---
layout: default
pagination:
enabled: true
---
<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<div class="home">

  <div class="site-header-container {% if site.cover %}has-cover{% endif %}"
    {% if site.cover %}style="background-image: url({{ site.cover | prepend: site.baseurl }});" {% endif %}>
    <div class="scrim {% if site.cover %}has-cover{% endif %}">
      <header class="site-header">
        <h1 class="title" style=font-size:50px>{{ site.title }}</h1>
        {% if site.subtitle %}<p class="title" style=font-size:30px>{{ site.subtitle }}</p>{% endif %}
      </header>
    </div>
  </div>
  <br>
  <div class="wrapper">
    <h1 id="headings">Introduction</h1>
    <p style="text-align: justify">
      Recently, video frame interpolation using a combination of frame- and event-based cameras has surpassed traditional image-based methods both in terms of performance and memory efficiency. However, current methods still suffer from (i) brittle image-level fusion of complementary interpolation results, that fails in the presence of artifacts in the fused image, (ii) potentially temporally inconsistent and inefficient motion estimation procedures, that run for every inserted frame and (iii) low contrast regions that do not trigger events, and thus cause events-only motion estimation to generate artifacts. Moreover, previous methods were only tested on datasets consisting of planar and faraway scenes, which do not capture the full complexity of the real world. In this work, we address the above problems by introducing multi-scale feature-level fusion and computing one-shot non-linear inter-frame motion—which can be efficiently sampled for image warping—from events and images. We also collect the first large-scale events and frames dataset consisting of more than 100 challenging scenes with depth variations, captured with a new experimental setup based on a beamsplitter. We show that our method improves the reconstruction quality by up to 0.2 dB in terms of PSNR and up to 15% in LPIPS score.            
    </p>
    <div class="container" style="margin-top:30px;margin-bottom:30px;">
      <h2>Publication</h2>
      Please cite our paper.
    </p>
    S. Tulyakov, A. Bochicchio, D. Gehrig, S. Georgoulis, Y. Li, D. Scaramuzza, <b>"Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow and Multi-scale Fusion" </b>,
      IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 
      <br>
      <a href="http://rpg.ifi.uzh.ch/docs/CVPR22_Tulyakov.pdf">[PDF]</a> 
      <a href="https://youtu.be/wHyngj4RZ_E">[YouTube]</a> 
      <a href="https://github.com/uzh-rpg/timelens-pp/">[Dataset]</a> 
    </p>
<pre>
<code class="pre-scrollable" style="background:#ffffff;color:rgb(36, 21, 21);font-size:12px;padding:0px;border-width:0px;">
@article{Tulyakov21cvpr,
  title={Time Lens++: Event-based Frame Interpolation with Parametric Non-linear Flow and Multi-scale Fusion},
  author={Tulyakov, Stepan and Bochicchio, Alfredo and Gehrig, Daniel and Georgoulis, Stamatios and Li, Yuanyou and Scaramuzza, Davide},
  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}
</code>
</pre>

    </p>

    <h2 id="headings">People</h2> 
<div id="people">
  <div class="inline-block">
    <a href="https://www.linkedin.com/in/stepan-tulyakov-phd-a2324331/?originalSubdomain=ch">
    <img class="circular--square" src="{{site.baseurl}}/assets/people/stepan.jpg" style="width: 150px">
    <p style="text-align:center"><strong>Stepan Tulyakov</strong></p>
    </a>
  </div>

  <div class="inline-block">
    <a href="https://www.linkedin.com/in/alfredo-bochicchio/">
    <img class="circular--square" src="{{site.baseurl}}/assets/people/alfredo.jpg" style="width: 150px">
    <p style="text-align:center"><strong>Alfredo Bochicchio</strong></p>
    </a>
  </div>

  <div class="inline-block">
    <a href="https://danielgehrig18.github.io/">
    <img class="circular--square" src="{{site.baseurl}}/assets/people/avatar.jpg" style="width: 150px">
    <p style="text-align:center"><strong>Daniel Gehrig</strong></p>
    </a>
  </div>

  <div class="inline-block">
    <a href="https://www.linkedin.com/in/stamatios-georgoulis-4a041636">
    <img class="circular--square" src="{{site.baseurl}}/assets/people/stam.jpg" style="width: 150px">
    <p style="text-align:center"><strong>Stamatios Georgoulis</strong></p>
    </a>
  </div>

  <div class="inline-block">
    <a href="https://www.linkedin.com/in/yuanyouli/">
    <img class="circular--square" src="{{site.baseurl}}/assets/people/yuanyou.jpg" style="width: 150px">
    <p style="text-align:center"><strong>Yuanyou Li</strong></p>
    </a>
  </div>

  <div class="inline-block">
    <a href="http://rpg.ifi.uzh.ch/people_scaramuzza.html">
    <img class="circular--square" src="{{site.baseurl}}/assets/people/davide.png" style="width: 150px">
    <p style="text-align:center"><strong>Prof. Davide Scaramuzza</strong></p>
    </a>
  </div>
</div>

  </div>



</div>